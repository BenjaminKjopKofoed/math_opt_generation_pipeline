{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning GPT 3.5 for solving OR problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_TRAIN_DATA_PATH = '/workspaces/da-atlante-engine/atlante/data/sources/llm-for-or/task3_train_public.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert in Operations Research, Python programming language, PuLP library and JSON file format.\n",
    "I will give you a description in natural language of an optimization problem <problem_instance> and you just have to write a Python script using PuLP to solve that specific instance of the problem.\n",
    "Don't write anything before and after the code, it will be simple for me to parse your answer. \n",
    "Please remember to solve the problem with status = problem.solve(PULP_CBC_CMD(msg=0)) in order to suppress solver's log.\n",
    "The code must only print a single string to stdout, formatted in JSON, which contains the answers to the question I will give you in input. \n",
    "Use the <output_keys_dict> to format the stdout and use them as keys of the JSON output.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(FULL_TRAIN_DATA_PATH)\n",
    "train_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'question': 'A company is organizing a team-building event and needs to assign participants to different activities. They have a total of 100 employees. Activity A requires 5 employees a team, activity B requires 3 employees a team, and activity C requires 7 employees a team. The company has a total of 100 employees available for the event. The company has a limitation on the number of teams in activity B, which cannot exceed 20. The company wants to maximize participation teams and decides to allocate different weights to each activity: activity A has a weight of 3, activity B has a weight of 2, and activity C has a weight of 4. The objective is to maximize the total participation weighted by the assigned weights.',\n",
       "  'code': '# Import PuLP library\\nfrom pulp import *\\n\\n# Define the decision variables\\nnum_participants_A = LpVariable(\"NumParticipantsA\", lowBound=0, cat=\\'Integer\\') # number of participants in activity A\\nnum_participants_B = LpVariable(\"NumParticipantsB\", lowBound=0, upBound=20, cat=\\'Integer\\') # number of participants in activity B\\nnum_participants_C = LpVariable(\"NumParticipantsC\", lowBound=0, cat=\\'Integer\\') # number of participants in activity C\\n\\n# Define the question as a maximum or minimum problem\\nproblem = LpProblem(\"TeamBuildingEvent\", LpMaximize)\\n\\n# Define the objective function\\nobjective = 3 * num_participants_A + 2 * num_participants_B + 4 * num_participants_C\\nproblem += objective # maximize the total participation weighted by the assigned weights\\n\\n# Define the constraints\\nproblem += 5 * num_participants_A + 3 * num_participants_B + 7 * num_participants_C <= 100 # the total number of employees is 100\\n\\n# Solve the problem\\nstatus = problem.solve()\\n\\n# Output the answer\\nprint(\"## start solving\")\\nprint(\"The number of participants in activity A:\", num_participants_A.value())\\nprint(\"The number of participants in activity B:\", num_participants_B.value())\\nprint(\"The number of participants in activity C:\", num_participants_C.value())\\nprint(\"The total weighted participation:\", objective.value())\\nprint(\"## end solving\")\\n',\n",
       "  'results': {'The number of teams in activity A': '8.0',\n",
       "   'The number of teams in activity B': '20.0',\n",
       "   'The number of teams in activity C': '0.0',\n",
       "   'The total weighted participation': '64.0'}},\n",
       " {'id': 1,\n",
       "  'question': 'A company is organizing a team-building event and needs to assign participants to different activities. They have a total of 100 employees. The company has a total of 100 employees available for the event. Each participant in activity A consumes 2 units of snacks, each participant in activity B consumes 1.5 units of snacks, and each participant in activity C consumes 3 units of snacks. The company has 250 units of snacks available for the event. The company wants to maximize participation and decides to allocate different weights to each activity: activity A has a weight of 3, activity B has a weight of 2, and activity C has a weight of 4. The objective is to maximize the total participation weighted by the assigned weights.',\n",
       "  'code': '# Import PuLP library\\nfrom pulp import *\\n\\n# Define the decision variables\\nnum_participants_A = LpVariable(\"NumParticipantsA\", lowBound=0, cat=\\'Integer\\') # number of participants in activity A\\nnum_participants_B = LpVariable(\"NumParticipantsB\", lowBound=0, cat=\\'Integer\\') # number of participants in activity B\\nnum_participants_C = LpVariable(\"NumParticipantsC\", lowBound=0, cat=\\'Integer\\') # number of participants in activity C\\n\\n# Define the question as a maximum or minimum problem\\nproblem = LpProblem(\"TeamBuildingEvent\", LpMaximize)\\n\\n# Define the objective function\\nobjective = 3 * num_participants_A + 2 * num_participants_B + 4 * num_participants_C\\nproblem += objective # maximize the total participation weighted by the assigned weights\\n\\n# Define the constraints\\nproblem += num_participants_A + num_participants_B + num_participants_C <= 100 # the number of participants must be at most 100\\nproblem += 2 * num_participants_A + 1.5 * num_participants_B + 3 * num_participants_C <= 250 # the snack units consumed must be at most 250\\n\\n# Solve the problem\\nstatus = problem.solve()\\n\\n# Output the answer\\nprint(\"The number of participants in activity A:\", num_participants_A.value())\\nprint(\"The number of participants in activity B:\", num_participants_B.value())\\nprint(\"The number of participants in activity C:\", num_participants_C.value())\\nprint(\"The total participation weighted score:\", objective.value())\\n',\n",
       "  'results': {'The number of participants in activity A': '50.0',\n",
       "   'The number of participants in activity B': '0.0',\n",
       "   'The number of participants in activity C': '50.0',\n",
       "   'The total participation weighted score': '350.0'}}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_formatted = []\n",
    "for ex in train_data:\n",
    "    user_content = \"<problem_instance>: {} \\n <output_keys_dict> {}\".format(ex['question'], ex['results'].keys())\n",
    "    formatted_ex = {\"messages\": [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_content}, {\"role\": \"assistant\", \"content\": ex['code']}]}\n",
    "    data_formatted.append(formatted_ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': \"You are an expert in Operations Research, Python programming language, PuLP library and JSON file format.\\nI will give you a description in natural language of an optimization problem <problem_instance> and you just have to write a Python script using PuLP to solve that specific instance of the problem.\\nDon't write anything before and after the code, it will be simple for me to parse your answer.\\nThe code must only print a single string to stdout, formatted in JSON, which contains the answers to the question I will give you in input. \\nUse the <output_keys_dict> to format the stdout and use them as keys of the JSON output.\"},\n",
       "  {'role': 'user',\n",
       "   'content': \"<problem_instance>: A body builder buys pre prepared meals, a turkey dinner and a tuna salad sandwich. The turkey dinner contains 20 grams of protein, 30 grams of carbs, and 12 grams of fat. The tuna salad sandwich contains 18 grams of protein, 25 grams of carbs, and 8 grams of fat. The bodybuilder wants to get at least 150 grams of protein and 200 grams of carbs. In addition because the turkey dinner is expensive, at most 40% of the meals should be turkey dinner. How many of each meal should he eat if he wants to minimize his fat intake? \\n <output_keys_dict> dict_keys(['The number of turkey dinners', 'The number of tuna salads', 'Total grams of fat intake'])\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': '# Import PuLP library\\nfrom pulp import *\\n\\n# Define the decision variables\\nnum_turkey_dinners = LpVariable(\"NumTurkeyDinners\", lowBound=0, cat=\\'Integer\\') # number of turkey dinners\\nnum_tuna_salads = LpVariable(\"NumTunaSalads\", lowBound=0, cat=\\'Integer\\') # number of tuna salads\\n\\n# Define the question as a maximum or minimum problem\\nproblem = LpProblem(\"MealPlan\", LpMinimize)\\n\\n# Define the objective function\\nobjective = 12 * num_turkey_dinners + 8 * num_tuna_salads\\nproblem += objective # minimize the total grams of fat\\n\\n# Define the constraints\\nproblem += 20 * num_turkey_dinners + 18 * num_tuna_salads >= 150 # get at least 150 grams of protein\\nproblem += 30 * num_turkey_dinners + 25 * num_tuna_salads >= 200 # get at least 200 grams of carbs\\nproblem += num_turkey_dinners <= (num_turkey_dinners + num_tuna_salads) * 0.4 # at most 40% meals are turkey dinners\\n\\n# Solve the problem\\nstatus = problem.solve()\\n\\n# Output the answer\\nprint(\"The number of turkey dinners:\", num_turkey_dinners.value())\\nprint(\"The number of tuna salads:\", num_tuna_salads.value())\\nprint(\"Total grams of fat intake:\", objective.value())'}]}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to remove\n",
    "ex_to_delate = [0,2,539, 634, 660, 661, 684, 687, 720, 723, 741, 768, 775, 793, 823, 831, 860, 910, 928, 940, 948, 999]\n",
    "new_list = [j for i, j in enumerate(data_formatted) if i not in ex_to_delate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_formatted = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path = '/workspaces/da-atlante-engine/atlante/data/sources/llm-for-or/full_data.jsonl'\n",
    "\n",
    "with open(full_data_path, 'w') as f:\n",
    "    for item in data_formatted:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 978\n",
      "First example:\n",
      "{'role': 'system', 'content': \"You are an expert in Operations Research, Python programming language, PuLP library and JSON file format.\\nI will give you a description in natural language of an optimization problem <problem_instance> and you just have to write a Python script using PuLP to solve that specific instance of the problem.\\nDon't write anything before and after the code, it will be simple for me to parse your answer.\\nThe code must only print a single string to stdout, formatted in JSON, which contains the answers to the question I will give you in input. \\nUse the <output_keys_dict> to format the stdout and use them as keys of the JSON output.\"}\n",
      "{'role': 'user', 'content': \"<problem_instance>: A company is organizing a team-building event and needs to assign participants to different activities. They have a total of 100 employees. The company has a total of 100 employees available for the event. Each participant in activity A consumes 2 units of snacks, each participant in activity B consumes 1.5 units of snacks, and each participant in activity C consumes 3 units of snacks. The company has 250 units of snacks available for the event. The company wants to maximize participation and decides to allocate different weights to each activity: activity A has a weight of 3, activity B has a weight of 2, and activity C has a weight of 4. The objective is to maximize the total participation weighted by the assigned weights. \\n <output_keys_dict> dict_keys(['The number of participants in activity A', 'The number of participants in activity B', 'The number of participants in activity C', 'The total participation weighted score'])\"}\n",
      "{'role': 'assistant', 'content': '# Import PuLP library\\nfrom pulp import *\\n\\n# Define the decision variables\\nnum_participants_A = LpVariable(\"NumParticipantsA\", lowBound=0, cat=\\'Integer\\') # number of participants in activity A\\nnum_participants_B = LpVariable(\"NumParticipantsB\", lowBound=0, cat=\\'Integer\\') # number of participants in activity B\\nnum_participants_C = LpVariable(\"NumParticipantsC\", lowBound=0, cat=\\'Integer\\') # number of participants in activity C\\n\\n# Define the question as a maximum or minimum problem\\nproblem = LpProblem(\"TeamBuildingEvent\", LpMaximize)\\n\\n# Define the objective function\\nobjective = 3 * num_participants_A + 2 * num_participants_B + 4 * num_participants_C\\nproblem += objective # maximize the total participation weighted by the assigned weights\\n\\n# Define the constraints\\nproblem += num_participants_A + num_participants_B + num_participants_C <= 100 # the number of participants must be at most 100\\nproblem += 2 * num_participants_A + 1.5 * num_participants_B + 3 * num_participants_C <= 250 # the snack units consumed must be at most 250\\n\\n# Solve the problem\\nstatus = problem.solve()\\n\\n# Output the answer\\nprint(\"The number of participants in activity A:\", num_participants_A.value())\\nprint(\"The number of participants in activity B:\", num_participants_B.value())\\nprint(\"The number of participants in activity C:\", num_participants_C.value())\\nprint(\"The total participation weighted score:\", objective.value())\\n'}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open(full_data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 445, 1096\n",
      "mean / median: 612.3098159509202, 595.5\n",
      "p5 / p95: 516.0, 738.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 201, 562\n",
      "mean / median: 273.5838445807771, 263.0\n",
      "p5 / p95: 225.7, 330.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~598839 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~1796517 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_formatted)\n",
    "\n",
    "train_data = data_formatted[:800]\n",
    "validation_data = data_formatted[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '/workspaces/da-atlante-engine/atlante/data/sources/llm-for-or/train_data.jsonl'\n",
    "\n",
    "with open(train_data_path, 'w') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_path = '/workspaces/da-atlante-engine/atlante/data/sources/llm-for-or/validation_data.jsonl'\n",
    "\n",
    "with open(validation_data_path, 'w') as f:\n",
    "    for item in validation_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "# Train upload\n",
    "\n",
    "train_file_openai = client.files.create(\n",
    "  file=open(train_data_path, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation upload\n",
    "\n",
    "validation_file_openai = client.files.create(\n",
    "  file=open(validation_data_path, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-jSSVeg8jjqcPZwx18mkcPHId', created_at=1716902661, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=4, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-GBbGKB5R2ohK7iGc0XliSbnr', result_files=[], seed=1117995693, status='validating_files', trained_tokens=None, training_file='file-FJmekHS1TFMf0FHCbpyfiY5u', validation_file='file-4rVYmEfHQbg5hwPwaSywQYjN', estimated_finish=None, integrations=[], user_provided_suffix='llm-for-or-new')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=train_file_openai.id, \n",
    "  validation_file=validation_file_openai.id,\n",
    "  model=\"gpt-3.5-turbo-0125\",\n",
    "  hyperparameters= {\"n_epochs\": 4},\n",
    "  suffix='llm-for-or-new'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-OM5BsmvtBPQHcuGmcv5J7rmw', created_at=1716903221, level='info', message='The job has successfully completed', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-fMrpBAGXUqd8SreXSYqU80mm', created_at=1716903214, level='info', message='New fine-tuned model created: ft:gpt-3.5-turbo-0125:spindox-spa:llm-for-or-new:9TrFGGza', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-Z8qLUO8GjTEWCgL1jcQYoha1', created_at=1716903214, level='info', message='Checkpoint created at step 1200 with Snapshot ID: ft:gpt-3.5-turbo-0125:spindox-spa:llm-for-or-new:9TrFF4a8:ckpt-step-1200', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-owuP3qF1BYaVjKI52gyWjtJO', created_at=1716903214, level='info', message='Checkpoint created at step 800 with Snapshot ID: ft:gpt-3.5-turbo-0125:spindox-spa:llm-for-or-new:9TrFFQZR:ckpt-step-800', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-c8JWxDBgNnO1UhwsVzvSWF68', created_at=1716903208, level='info', message='Step 1600/1600: training loss=0.01, validation loss=0.06, full validation loss=0.06', object='fine_tuning.job.event', data={'step': 1600, 'train_loss': 0.005081682465970516, 'valid_loss': 0.0569187489467946, 'total_steps': 1600, 'full_valid_loss': 0.055078251957857545, 'train_mean_token_accuracy': 1.0, 'valid_mean_token_accuracy': 0.9833679833679834, 'full_valid_mean_token_accuracy': 0.9854941796397587}, type='metrics'), FineTuningJobEvent(id='ftevent-aSARNFwedv3BnZHilResleVR', created_at=1716903202, level='info', message='Step 1599/1600: training loss=0.08', object='fine_tuning.job.event', data={'step': 1599, 'train_loss': 0.07867204397916794, 'total_steps': 1600, 'train_mean_token_accuracy': 0.9867924451828003}, type='metrics'), FineTuningJobEvent(id='ftevent-gU0Rs5FEpY3hnYSkAEoVKonG', created_at=1716903198, level='info', message='Step 1598/1600: training loss=0.03', object='fine_tuning.job.event', data={'step': 1598, 'train_loss': 0.025128137320280075, 'total_steps': 1600, 'train_mean_token_accuracy': 0.9864077568054199}, type='metrics'), FineTuningJobEvent(id='ftevent-KWdSAi6XyPbUtQoJEbSkK1Jq', created_at=1716903196, level='info', message='Step 1597/1600: training loss=0.00', object='fine_tuning.job.event', data={'step': 1597, 'train_loss': 0.0037638687063008547, 'total_steps': 1600, 'train_mean_token_accuracy': 0.9982078671455383}, type='metrics'), FineTuningJobEvent(id='ftevent-SPBGBWNV9bf4tlkaKumRLNn1', created_at=1716903194, level='info', message='Step 1596/1600: training loss=0.01', object='fine_tuning.job.event', data={'step': 1596, 'train_loss': 0.007377018686383963, 'total_steps': 1600, 'train_mean_token_accuracy': 0.9981203079223633}, type='metrics'), FineTuningJobEvent(id='ftevent-etiUWTLQFqf4zQRPEXupjH9A', created_at=1716903194, level='info', message='Step 1595/1600: training loss=0.01', object='fine_tuning.job.event', data={'step': 1595, 'train_loss': 0.005919009912759066, 'total_steps': 1600, 'train_mean_token_accuracy': 0.9980879426002502}, type='metrics')], object='list', has_more=True)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this line for checking status\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-hQYrRfDMEw4CrYCczMIZRGO1\", limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/workspaces/da-atlante-engine/atlante/data/sources/llm-for-or/task3_test_public.json')\n",
    "test_data = json.load(f)\n",
    "\n",
    "data_test_formatted = []\n",
    "for ex in test_data:\n",
    "    user_content = \"<problem_instance>: {} \\n <output_keys_dict> {}\".format(ex['question'], ex['results'].keys())\n",
    "    formatted_ex = {\"messages\": [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_content}]}\n",
    "    data_test_formatted.append(formatted_ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change it for running on a new test istance\n",
    "index = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': \"You are an expert in Operations Research, Python programming language, PuLP library and JSON file format.\\nI will give you a description in natural language of an optimization problem <problem_instance> and you just have to write a Python script using PuLP to solve that specific instance of the problem.\\nDon't write anything before and after the code, it will be simple for me to parse your answer.\\nThe code must only print a single string to stdout, formatted in JSON, which contains the answers to the question I will give you in input. \\nUse the <output_keys_dict> to format the stdout and use them as keys of the JSON output.\"},\n",
       "  {'role': 'user',\n",
       "   'content': \"<problem_instance>: A furniture factory makes two products: bedside tables and bookcases. Both products have to go through two processes: crafting and polishing. For each bedside table, the workers spend 2.5 hours crafting and 1.5 hours polishing. For each bookcase, the workers spend 5 hours crafting and 3 hours polishing. On any day, there is a maximum of 30 crafting hours available and 20 polishing hours available. The profit from the sale of each bedside table is $200 and the profit from the sale of each bookcase is $500. The factory can sell everything they make. How should they schedule daily production in order to maximize profit? \\n <output_keys_dict> dict_keys(['The number of bedside tables produced', 'The number of bookcases produced', 'The total profit'])\"}]}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_formatted[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft:gpt-3.5-turbo-0125:spindox-spa:llm-for-or:9TouHarF   ---> first fine tuning\n",
    "# ft:gpt-3.5-turbo-0125:spindox-spa:llm-for-or-new:9TrFGGza ---> last fine tuning\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:spindox-spa:llm-for-or-new:9TrFGGza\",\n",
    "  messages=data_test_formatted[index]['messages']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Import PuLP library\\nfrom pulp import *\\n\\n# Define the decision variables\\nnum_bedside_tables = LpVariable(\"NumBedsideTables\", lowBound=0, cat=\\'Continuous\\') # number of bedside tables produced\\nnum_bookcases = LpVariable(\"NumBookcases\", lowBound=0, cat=\\'Continuous\\') # number of bookcases produced\\n\\n# Define the question as a maximum or minimum problem\\nproblem = LpProblem(\"FurnitureFactory\", LpMaximize)\\n\\n# Define the objective function\\nobjective = 200 * num_bedside_tables + 500 * num_bookcases\\nproblem += objective # maximize the total profit\\n\\n# Define the constraints\\nproblem += 2.5 * num_bedside_tables + 5 * num_bookcases <= 30 # crafting hours constraint\\nproblem += 1.5 * num_bedside_tables + 3 * num_bookcases <= 20 # polishing hours constraint\\n\\n# Solve the problem\\nstatus = problem.solve(PULP_CBC_CMD(msg=0))\\n\\n# Output the answer\\nprint(\"The number of bedside tables produced:\", num_bedside_tables.value())\\nprint(\"The number of bookcases produced:\", num_bookcases.value())\\nprint(\"The total profit:\", objective.value())\\n'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.replace('problem.solve()', 'problem.solve(PULP_CBC_CMD(msg=0))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlante-aabxrmYf-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
